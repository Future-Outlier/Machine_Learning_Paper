1.架構
2.接下來講了一些 BERT 的 DETAILS 
1.transformer architechture
2.Two training objectives //簡單帶過  1分鐘
(1) masked language model (介紹方法)填空題看答案 // 簡單帶過 1分鐘 
(2) next sentence pre-diction (介紹方法)預測下一句 // 簡單帶過 1分鐘
3.Optimization, GELU activation funciotn // Adam調整，GELU激活
4.Data  BOOKCORPUS (語料庫) (某來源) + 維基百科英文(總共16GB 非壓縮text)

3.介紹背景

4.DATA 來源
1.BOOKCORPUS (Zhu et al., 2015) plus English
WIKIPEDIA. This is the original data used to
train BERT. (16GB).
2.CC-NEWS, which we collected from the English 
portion of the CommonCrawl News
dataset (Nagel, 2016). The data contains 63
million English news articles crawled between
September 2016 and February 2019. (76GB after filtering).
3.OPENWEBTEXT (Gokaslan and Cohen, 2019),
an open-source recreation of the WebText corpus described
in Radford et al. (2019). The text is web content extracted from
URLs shared on
Reddit with at least three upvotes. (38GB).
4.STORIES, a dataset introduced in Trinh and Le
(2018) containing a subset of CommonCrawl
data filtered to match the story-like style of
Winograd schemas. (31GB).

5. GLUE, RACE and SQuAD 大家的任務介紹 (簡短介紹)
ReAding Comprehension Dataset From Examinations


5. 四大 training tips 更動